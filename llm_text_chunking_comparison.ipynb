{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ ðŸ“„ **Based on my analysis â€” LLM Text Chunking: Strategy Comparison & Use Cases**  \n",
    "*This project demonstrates and compares various text chunking strategies for Large Language Models (LLMs).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Library / Tool**   | **Best Use Case**                                   | **Why Use It**                                          | **Preferred When**                                                                   | **Example Code / Notes**                                                        |\n",
    "|----------------------|-----------------------------------------------------|---------------------------------------------------------|--------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| **NLTK**             | Rule-based chunking for academic or structured text | Lightweight, easy to use, sentence tokenization         | Working with academic/research texts needing sentence/paragraph chunking based on rules | `from nltk.tokenize import sent_tokenize`<br>`sent_tokenize(text)`               |\n",
    "| **spaCy**            | Linguistic chunking (sentences, phrases, POS, NER)  | Fast NLP pipeline with robust sentence segmentation     | High-performance NLP chunking with linguistic awareness and production usage          | `nlp = spacy.load(\"en_core_web_sm\")`<br>`[sent.text for sent in nlp(text).sents]`|\n",
    "| **Gensim**           | Chunking for large corpus and topic modeling        | Efficient for large datasets and topic modeling         | Handling large datasets, corpus-based processing, similarity analysis                 | Custom token-based preprocessing                                                  |\n",
    "| **LangChain**        | Recursive semantic-aware splitting for RAG          | Smart fallback (paragraph â†’ sentence â†’ word â†’ char)     | Need best-effort semantic chunking for Retrieval-Augmented Generation (RAG) pipelines | `RecursiveCharacterTextSplitter(chunk_size=500)`                                 |\n",
    "| **Transformers**     | Token-based chunking for model input formatting     | Precise handling of transformer model input limits      | Chunking text exactly for Hugging Face models like BERT, GPT, T5                      | `PreTrainedTokenizerBase.encode_plus()`                                          |\n",
    "| **Haystack**         | Chunking documents in QA / NLP pipelines            | Integrated with retriever-reader systems, clean pipelines | Using Haystack framework with Elasticsearch, retrievers, and readers                  | `from haystack.nodes import TextConverter, PreProcessor`                          |\n",
    "| **Tiktoken**         | Token counting for OpenAI models                    | Accurate token budgeting for ChatGPT, GPT-4, etc.       | Need precise token control while chunking text for OpenAI model requests              | `tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode(text)`                     |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
